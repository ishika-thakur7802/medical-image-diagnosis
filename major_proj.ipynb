{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPJR0IntldAhs929P0QlNWg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ishika-thakur7802/medical-image-diagnosis/blob/main/major_proj.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kSXl4E9l8D_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c318aa28-19a0-4abd-e315-cbcdb3b94d36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Input, Conv2D, GlobalAveragePooling2D, GlobalMaxPooling2D, Dense, Add, Multiply, Activation, Reshape, Flatten\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Mo6CpFDu_VOw"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "# Define CLAHE function\n",
        "def apply_clahe(input_img):\n",
        "    lab = cv2.cvtColor(input_img, cv2.COLOR_RGB2LAB)  # Convert to LAB color space\n",
        "    l, a, b = cv2.split(lab)  # Split into Luminance, A, and B channels\n",
        "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
        "    cl = clahe.apply(l)  # Apply CLAHE on the Luminance channel\n",
        "    lab = cv2.merge((cl, a, b))  # Merge channels back\n",
        "    enhanced_img = cv2.cvtColor(lab, cv2.COLOR_LAB2RGB)  # Convert back to RGB\n",
        "    return enhanced_img\n",
        "\n",
        "# Define image preprocessing function\n",
        "def preprocess_image(image_path, label, use_clahe=True):\n",
        "    input_img = cv2.imread(image_path)\n",
        "    if input_img is None:\n",
        "        print(f\"Warning: Image {os.path.basename(image_path)} could not be loaded.\")\n",
        "        return None, None\n",
        "    input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Apply CLAHE if specified\n",
        "    if use_clahe:\n",
        "        input_img = apply_clahe(input_img)\n",
        "\n",
        "    # Resize and normalize\n",
        "    input_img_resize = cv2.resize(input_img, (224, 224))\n",
        "    input_img_resize = input_img_resize / 255.0  # Normalize to [0, 1]\n",
        "    return input_img_resize, label\n",
        "\n",
        "# Define function to load and preprocess data from a folder\n",
        "def load_data_from_folder(folder_path, label, use_clahe=True):\n",
        "    img_data_list = []\n",
        "    labels = []\n",
        "    for dataset in os.listdir(folder_path):\n",
        "        img, lbl = preprocess_image(os.path.join(folder_path, dataset), label, use_clahe)\n",
        "        if img is not None:\n",
        "            img_data_list.append(img)\n",
        "            labels.append(lbl)\n",
        "    return img_data_list, labels\n",
        "\n",
        "# Load and preprocess dataset\n",
        "data_path_0 = '/content/drive/MyDrive/major-project/OS Collected Data/Normal'  # Class 0\n",
        "data_path_1 = '/content/drive/MyDrive/major-project/OS Collected Data/Osteopenia'  # Class 1\n",
        "data_path_2 = '/content/drive/MyDrive/major-project/OS Collected Data/Osteoporosis'  # Class 2\n",
        "\n",
        "\n",
        "# Load class 0 data\n",
        "img_data_0, labels_0 = load_data_from_folder(data_path_0, label=0, use_clahe=True)\n",
        "\n",
        "# Load class 1 data\n",
        "img_data_1, labels_1 = load_data_from_folder(data_path_1, label=1, use_clahe=True)\n",
        "\n",
        "# Load class 2 data\n",
        "img_data_2, labels_2 = load_data_from_folder(data_path_2, label=2, use_clahe=True)\n",
        "\n",
        "# Combine data and labels\n",
        "img_data_list = img_data_0 + img_data_1+img_data_2\n",
        "labels = labels_0 + labels_1+labels_2\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X = np.array(img_data_list, dtype=np.float32)\n",
        "y = np.array(labels)\n",
        "\n",
        "# Shuffle data\n",
        "X, y = shuffle(X, y, random_state=42)\n",
        "\n",
        "# Print dataset summary\n",
        "print(f\"Total images: {X.shape[0]}\")\n",
        "print(f\"Image shape: {X.shape[1:]}\")\n",
        "print(f\"Labels distribution: {np.unique(y, return_counts=True)}\")"
      ],
      "metadata": {
        "id": "2Ymgf0qW_YQL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2ba36e41-83a5-46bd-a4cc-b11d340906a1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 1947\n",
            "Image shape: (224, 224, 3)\n",
            "Labels distribution: (array([0, 1, 2]), array([780, 374, 793]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Number of images: {len(img_data_list)}')\n",
        "print(f'Number of labels: {len(labels)}')\n"
      ],
      "metadata": {
        "id": "ojNy_ul__fPU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea6976d-f3fc-411f-bcfa-0fe63d89532d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of images: 1947\n",
            "Number of labels: 1947\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert lists to arrays\n",
        "img_data = np.array(img_data_list)\n",
        "labels = np.array(labels)\n",
        "\n",
        "print(f'Image data shape: {img_data.shape}')\n",
        "print(f'Labels shape: {labels.shape}')\n"
      ],
      "metadata": {
        "id": "_nfcHzXp_f8X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99eede97-7322-4f82-e29a-a38039c71b5f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Image data shape: (1947, 224, 224, 3)\n",
            "Labels shape: (1947,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the labels\n",
        "labels = to_categorical(labels, num_classes=3)"
      ],
      "metadata": {
        "id": "qbVO52qs_i35"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Labels shape after one-hot encoding: {labels.shape}')"
      ],
      "metadata": {
        "id": "LvhpO1OC_luP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1a2abbd4-61d5-4767-b4f3-59d2811b26d9"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Labels shape after one-hot encoding: (1947, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the data into training and validation sets\n",
        "X_train, X_val, y_train, y_val = train_test_split(img_data, labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "atEIJAPn_odt"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'X_train shape after split: {X_train.shape}')\n",
        "print(f'y_train shape after split: {y_train.shape}')\n",
        "print(f'X_val shape after split: {X_val.shape}')\n",
        "print(f'y_val shape after split: {y_val.shape}')"
      ],
      "metadata": {
        "id": "6xTNzeta_qxy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31c3dfa0-e8f6-4933-d326-bbe299729bc2"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape after split: (1557, 224, 224, 3)\n",
            "y_train shape after split: (1557, 3)\n",
            "X_val shape after split: (390, 224, 224, 3)\n",
            "y_val shape after split: (390, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout\n",
        "from tensorflow.keras.applications import ResNet50\n",
        "\n",
        "# Load a pre-trained ResNet model (excluding the top layer)\n",
        "base_model = ResNet50(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "\n",
        "# Freeze the base model layers\n",
        "base_model.trainable = False\n",
        "\n",
        "# Build your model\n",
        "model = Sequential([\n",
        "    base_model,  # Pre-trained ResNet base model\n",
        "    Flatten(),\n",
        "    Dense(512, activation='relu'),\n",
        "    Dropout(0.5),  # Regularization to prevent overfitting\n",
        "    Dense(3, activation='softmax')  # 3 classes (healthy, osteopenia, osteoporosis)\n",
        "])\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Summary of the model\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "ILYNwxh4MJJC",
        "outputId": "96c28b42-2c37-4245-a483-876bddc30953"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)          │      \u001b[38;5;34m23,587,712\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100352\u001b[0m)              │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │      \u001b[38;5;34m51,380,736\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)                    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m512\u001b[0m)                 │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3\u001b[0m)                   │           \u001b[38;5;34m1,539\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)          │      <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100352</span>)              │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │      <span style=\"color: #00af00; text-decoration-color: #00af00\">51,380,736</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)                    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)                 │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)                   │           <span style=\"color: #00af00; text-decoration-color: #00af00\">1,539</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m74,969,987\u001b[0m (285.99 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">74,969,987</span> (285.99 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m51,382,275\u001b[0m (196.01 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">51,382,275</span> (196.01 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m23,587,712\u001b[0m (89.98 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> (89.98 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Create an ImageDataGenerator instance for augmentation\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "# Fit the generator on the training data\n",
        "datagen.fit(X_train)\n"
      ],
      "metadata": {
        "id": "QlQ9KB0DMmJd"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                    epochs=10,\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    steps_per_epoch=len(X_train) // 32)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DQGMcUlkNExC",
        "outputId": "8af83c35-3242-4586-8484-dce1f94e2b3c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:122: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m54s\u001b[0m 748ms/step - accuracy: 0.3957 - loss: 11.5997 - val_accuracy: 0.5462 - val_loss: 0.9032\n",
            "Epoch 2/10\n",
            "\u001b[1m 1/48\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 130ms/step - accuracy: 0.4062 - loss: 1.0765"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/lib/python3.11/contextlib.py:158: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
            "  self.gen.throw(typ, value, traceback)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 29ms/step - accuracy: 0.4062 - loss: 1.0765 - val_accuracy: 0.6051 - val_loss: 0.8989\n",
            "Epoch 3/10\n",
            "\u001b[1m48/48\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 339ms/step - accuracy: 0.5319 - loss: 1.0359"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Unfreeze the top layers of the base model\n",
        "base_model.trainable = True\n",
        "\n",
        "# Freeze the first 10 layers and unfreeze the remaining layers\n",
        "for layer in base_model.layers[:10]:\n",
        "    layer.trainable = False\n",
        "\n",
        "# Re-compile the model (required after modifying trainable layers)\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "              loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Continue training\n",
        "history_finetune = model.fit(datagen.flow(X_train, y_train, batch_size=32),\n",
        "                             epochs=10,\n",
        "                             validation_data=(X_val, y_val),\n",
        "                             steps_per_epoch=len(X_train) // 32)"
      ],
      "metadata": {
        "id": "guVJjPTuSIEK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Re-evaluate the model after fine-tuning\n",
        "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
        "print(f\"Test accuracy after fine-tuning: {test_acc:.4f}\")\n"
      ],
      "metadata": {
        "id": "cBa3MWw0SK-Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}